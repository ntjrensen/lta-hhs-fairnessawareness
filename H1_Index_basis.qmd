---
title: "Prognosemodel {{< meta params.model >}}"
subtitle: "`r params$faculteit` | `r params$opleidingsnaam` (`r params$opleiding`) - `r params$opleidingsvorm` - versie `r params$versie`"

# Auteur en datum
author: "Theo Bakker, lector Learning Technology & Analytics, De HHs"
date: last-modified

# LTA Template
ltatemplate: 0.9.1.9000

# Parameters
params:
  versie: "1.0"
  succes: "Retentie na 1 jaar"
  model: "Retentie na 1 jaar"
  propedeusediploma: "Nvt"
  use_synthetic_data: true
  recreateplots: false

  faculteit: "ITD"
  opleidingsnaam: "B Communication and Multimedia Design"
  opleiding: "CMD"
  opleidingsvorm: "voltijd"
  opleidingsvorm_afkorting: "VT"
  selectie: false

# Format en output
output-file: "lta-hhs-tidymodels-h1-basis.html"

# Content
includes:
  inleiding:      true
  data:           true
  model_lr:       true
  model_rf:       true
  model_svm:      false
  final_fit:      true
  conclusies:     true
  verantwoording: true
  nextsteps:      true
  copyright:      true
---

<!-- Title -->

```{r setup, include = FALSE}
#| label: setup

prognose_titel <- paste("Prognosemodel", tolower(params$model)) ## Set the title

source("_Setup.R") ## Sluit het _Setup.R bestand in

bInclude_Model_LR    <- rmarkdown::metadata$includes$model_lr    ## Penalized Logistic Regression
bInclude_Model_RF    <- rmarkdown::metadata$includes$model_rf    ## Random Forest
bInclude_Model_SVM   <- rmarkdown::metadata$includes$model_svm   ## Support Vector Machine
bInclude_Final_fit   <- rmarkdown::metadata$includes$final_fit   ## Final Fit
bInclude_Conclusies  <- rmarkdown::metadata$includes$conclusies  ## Conclusies

dfModel_results <- data.frame(
  model = character(),
  auc = numeric()
)
```

<!-- Methode -->

## Methode, data en analyse

### Toelichting op de methode

Voor de ontwikkeling van prognosemodellen gebruiken we de aanpak van [Tidymodels](https://www.tidymodels.org/). Tidymodels is een framework voor het bouwen van een prognosemodel. Hiermee verzekeren we ons van een systematische, herhaalbare en schaalbare aanpak.

### Toelichting op de data

De basis voor deze analyse is studiedata van De Haagse Hogeschool (De HHs), verrijkt door het lectoraat LTA. De data bevat informatie over de inschrijvingen van studenten in het eerste jaar van de opleiding:

1.  *Demografische kenmerken*: geslacht, leeftijd, reistijd en SES totaalscore.
2.  *Vooropleidingskenmerken*: toelaatgevende vooropleiding, studiekeuzeprofiel en gemiddeld eindcijfer in de vooropleiding.
3.  *Aanmeldingskenmerken*: aansluiting (direct na diploma, tussenjaar, switch), dag van aanmelding, aantal parallelle studies aan De HHs en collegejaar.

Deze variabelen zijn gekozen omdat we uit eerder onderzoek weten dat ze voorspellende waarde kunnen hebben voor studiesucces [@Bakker.2022] of omdat ze behoren tot sensitieve kenmerken die in fairness analyse gebruikt worden.

```{r}
#| label: tbl-data-dictionary
#| tbl-cap: "Variabelen en mogelijke waarden"

# Lees de data dictionary in
dfData_dictionary <- Get_Data_Dictionary()

# Toon de data dictionary
Get_tblData_Dictionary(dfData_dictionary)
  

```

### Toelichting op de analyse

We toetsen in deze analyse *`r sSucces_model_text`*, voortaan **Retentie** genoemd.

Retentie is gedefinieerd als ingeschreven staan in dezelfde opleiding in een aansluitend collegejaar. Een wisseling van opleidingsvorm binnen de opleiding, bijvoorbeeld van voltijd in jaar 1 naar duaal in jaar 2, geldt ook als retentie. Uitval is het tegenovergestelde van retentie: niet ingeschreven staan in dezelfde opleiding in een aansluitend collegejaar. Een wisseling van opleidingsvorm binnen de opleiding, bijvoorbeeld van voltijd in jaar 1 naar duaal in jaar 2, geldt *niet* als uitval.

<!-- Data -->

## Voorbereidingen

### Laad de data

We laden een subset in van historische data specifiek voor:

**Opleiding**: `r params$faculteit` \| `r Get_Opleidingsnaam_Synth(params$opleidingsnaam)` (`r params$opleiding`), `r params$opleidingsvorm`, eerstejaars - **`r sSucces_model`**

```{r}
#| label: load-data

## Laad de data voor de opleiding
if(params$use_synthetic_data) {
  dfOpleiding_inschrijvingen_base <- Get_Studyprogram_Enrollments_Synthetic(
    studytrack = params$opleiding,
    studyform = toupper(params$opleidingsvorm_afkorting)
  ) |> 
    mutate(
      INS_Student_UUID_opleiding_vorm = paste(ID, INS_Opleiding, INS_Opleidingsvorm, sep = "_"),
      INS_Opleidingsnaam_huidig = paste(INS_Opleidingsnaam_huidig, "(Synth.)", sep = " ")
    )
} else {
  dfOpleiding_inschrijvingen_base <- get_lta_studyprogram_enrollments_pin(
    board = "HHs/Inschrijvingen",
    faculty = params$faculteit,
    studyprogram = params$opleidingsnaam,
    studytrack = params$opleiding,
    studyform = toupper(params$opleidingsvorm),
    range = "eerstejaars")
}

## Herschik de levels
Set_Levels(dfOpleiding_inschrijvingen_base)

dfOpleiding_inschrijvingen_base <- dfOpleiding_inschrijvingen_base |>  
  
  ## Maak een eenvoudige succes variabele aan
  Mutate_Retentie(sSucces_model) |>
  
  ## Maak van de succes variabele een factor
  mutate(SUC_Retentie = as.factor(SUC_Retentie)) |> 

  ## Verbijzonder eventueel op basis van het propedeusediploma
  # Filter_Propedeusediploma(sPropedeusediploma) |>

  ## Maak van de Dubbele studie variabele een Ja/Nee variabele
  mutate(INS_Dubbele_studie = ifelse(INS_Aantal_inschrijvingen > 1, "Ja", "Nee")) |>  

  ## Verwijder INS_Aantal_inschrijvingen
  select(-INS_Aantal_inschrijvingen) |> 

  ## Pas voor een aantal variabelen de levels aan
  Mutate_Levels(
  c(
    "VOP_Studiekeuzeprofiel_LTA_afkorting",
    "INS_Aansluiting_LTA",
    "VOP_Toelaatgevende_vooropleiding_soort"
  ),
    list(lLevels_skp, lLevels_vop, lLevels_vop)
  )
  
## B Huidtherapie: Filter op uitsluitend studenten met een rangnummer (selectie)
if(opleiding == "HDT") {
  dfOpleiding_inschrijvingen_base <- dfOpleiding_inschrijvingen_base |> 
    filter(!is.na(RNK_Rangnummer)) 
} 

```

### Selecteer en inspecteer de data

We selecteren eerst de relevante variabelen. We verwijderen daarbij variabelen die maar 1 waarde hebben. We inspecteren de variabelen in een samenvatting in relatie tot retentie en corrigeren daarbij voor multiple testing; de gecorrigeerde significantie-waarden staan vermeld als *q-value*. Daarnaast inspecteren we de kwaliteit van de data op missende waarden.

```{r}
#| label: select-inspect-data

lSelect <- c(
    "INS_Student_UUID_opleiding_vorm",
    "CBS_APCG_tf",
    "DEM_Geslacht",
    "DEM_Leeftijd_1_oktober",
    "GIS_Tijd_fiets_OV",
    "INS_Collegejaar",
    "INS_Dagen_tussen_aanmelding_en_1_september",
    "INS_Dubbele_studie",
    "INS_Aansluiting_LTA",
    "SES_Deelscore_arbeid",
    "SES_Deelscore_welvaart",
    "SES_Totaalscore",
    "SUC_Retentie",
    "VOP_Gemiddeld_cijfer_cijferlijst",
    "VOP_Gemiddeld_eindcijfer_VO_van_de_hoogste_vooropleiding_voor_het_HO",
    "VOP_Cijfer_CE1_nederlands",
    "VOP_Cijfer_CE1_engels",
    "VOP_Cijfer_CE_proxy_wiskunde",
    "VOP_Cijfer_CE1_natuurkunde",
    "VOP_Studiekeuzeprofiel_LTA_afkorting",
    "VOP_Toelaatgevende_vooropleiding_soort"
  )

## B Huidtherapie: voeg de variabele RNK_Rangnummer toe
if(opleiding == "HDT") {
  lSelect <- c(lSelect, "RNK_Rangnummer")
}

## Maak een subset
dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen_base |>
  
  ## Selecteer de relevante variabelen
  select_at(lSelect) |>
  
  ## Hernoem variabelen voor beter leesbare namen
  rename(
    ID                    = INS_Student_UUID_opleiding_vorm,
    Geslacht              = DEM_Geslacht,
    Leeftijd              = DEM_Leeftijd_1_oktober,
    Reistijd              = GIS_Tijd_fiets_OV,
    Dubbele_studie        = INS_Dubbele_studie,
    Collegejaar           = INS_Collegejaar,
    Aanmelding            = INS_Dagen_tussen_aanmelding_en_1_september,
    Aansluiting           = INS_Aansluiting_LTA,
    APCG                  = CBS_APCG_tf,
    SES_Arbeid            = SES_Deelscore_arbeid,
    SES_Welvaart          = SES_Deelscore_welvaart,
    SES_Totaal            = SES_Totaalscore,          
    Retentie              = SUC_Retentie,
    Cijfer_SE_VO          = VOP_Gemiddeld_cijfer_cijferlijst,
    Cijfer_CE_VO          = VOP_Gemiddeld_eindcijfer_VO_van_de_hoogste_vooropleiding_voor_het_HO,
    Cijfer_CE_Nederlands  = VOP_Cijfer_CE1_nederlands,
    Cijfer_CE_Engels      = VOP_Cijfer_CE1_engels,
    Cijfer_CE_Wiskunde    = VOP_Cijfer_CE_proxy_wiskunde,
    Cijfer_CE_Natuurkunde = VOP_Cijfer_CE1_natuurkunde,
    Studiekeuzeprofiel    = VOP_Studiekeuzeprofiel_LTA_afkorting,
    Vooropleiding         = VOP_Toelaatgevende_vooropleiding_soort
  ) |> 
  
  ## Pas CBS_APCG_tf aan naar factor
  mutate(APCG = case_when(APCG == TRUE ~ "Ja",
                          APCG == FALSE ~ "Nee",
                          .default = "Onbekend")) |>

  ## Geef aan waar missende cijfers in het VO zijn
  Mutate_Cijfers_VO() |>
  
  ## Verwijder variabelen, waarbij er maar 1 waarde is
  select(where(~ n_distinct(.) > 1)) |>
  
  arrange(Collegejaar, ID)

## B Huidtherapie: hernoem de variabele RNK_Rangnummer
if(opleiding == "HDT") {
  dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen |> 
    rename(Rangnummer = RNK_Rangnummer)
} 

dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen |> 
 ltabase::sort_distinct()

## Verwijder de basis dataset
## rm(dfOpleiding_inschrijvingen_base)

```

```{r, echo=FALSE, results='asis'}
#| label: tbl-summary-data
#| tbl-cap: "Variabelen in relatie tot de uitkomstmaat: params$succes"

## Maak een samenvatting van de data
dfSummary <- dfOpleiding_inschrijvingen |>
  
  ## Verwijder kolommen die niet relevant zijn voor de analyse
  select(-c(ID, Collegejaar)) |> 
  
  ## Pas de labels van Retentie aan van True naar Ja, en van False naar Nee
  mutate(Retentie = fct_recode(Retentie, "Nee" = "FALSE", "Ja" = "TRUE")) |>
  
  ## Pas de volgorde van de labels van Retentie aan
  mutate(Retentie = fct_relevel(Retentie, "Ja", "Nee")) |> 
  
  ## Pas studiekeuzeprofiel aan: als NA, dan onbekend
  mutate(Studiekeuzeprofiel = coalesce(Studiekeuzeprofiel, "Onbekend")) |> 
  
  ## Maak van alle character variabelen een factor
  mutate(across(where(is.character), as.factor))

## Maak een tbl_df van de samenvatting
tbl_dfSummary <- Get_tblSummary(dfSummary) 

tbl_dfSummary
  
```

```{r}
#| label: tbl-summarizy-missing-before
#| tbl-cap: "Kwaliteit van de data voor bewerkingen (gesorteerd op missende waarden)"

## Laad dlookr
suppressMessages(library(dlookr))

## Toon een samenvatting van de data, gesorteerd op missende waarden
diagnose(dfOpleiding_inschrijvingen) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(missing_percent, 2)) |>
  arrange(desc(missing_percent)) |>
  knitr::kable(col.names = c("Variabelen",
                           "Type",
                           "# Missende waarden",
                           "% Missende waarden",
                           "# Unieke waarden",
                           "Ratio unieke waarden"))

## Verwijder dlookr
detach("package:dlookr", unload = TRUE)

```

### Bewerk de data

-   Uit de eerste diagnose blijkt dat niet alle variabelen goed genoeg zijn voor het bouwen van een prognosemodel: er zijn missende waarden en niet alle veldtypes zijn geschikt.
-   Om bias te voorkomen verwijderen we geen rijen met missende waarden, maar vullen die op (*imputatie*). We bewerken de data zo dat alle missende waarden worden opgevuld: bij numerieke waarden met het gemiddelde en bij categorische variabelen met 'Onbekend'.
-   We passen het type van sommige variabelen aan, zodat ze in het model gebruikt kunnen worden: tekstvelden zetten we om naar factor (een categorische variabele); logische variabelen (Ja/Nee) zetten we om naar een numerieke variabele (1/0).
-   De uitkomstvariabele, `Retentie`, leiden we af van de variabele `SUC_Uitval_aantal_jaar_LTA`. Als de waarde daar 1 is, is de student na 1 jaar uitgevallen, 2 na 2 jaar, etc. Zolang de waarde daar 0 is, is de student niet uitgevallen.
-   Een fictief studentnummer (`INS_Student_UUID_opleiding_vorm`) gebruiken we, zodat we - als er afwijkende resultaten zijn - de dataset gericht kunnen onderzoeken als dat nodig is.

```{r}
#| label: tbl-summarizy-missing-after
#| tbl-cap: "Kwaliteit van de data na bewerkingen (gesorteerd op missende waarden)"

## Bewerk de data
dfOpleiding_inschrijvingen <- dfOpleiding_inschrijvingen |> 
  
  ## Imputeer alle numerieke variabelen met de mean
  mutate(across(where(is.numeric), ~ ifelse(
    is.na(.x),
    mean(.x, na.rm = T),
    .x
  )) ) |>
  
  ## Zet character variabelen om naar factor
  mutate(across(where(is.character), as.factor)) |> 
  
  ## Zet logische variabelen om naar 0 of 1
  mutate(across(where(is.logical), as.integer)) |>
  
  ## Vul in factoren missende waarden op met "Onbekend"
  mutate(across(where(is.factor), ~ suppressWarnings(
    fct_explicit_na(.x, na_level = "Onbekend")
  ))) |> 
  
  ## Herschik de kolommen, zodat Retentie vooraan staat
  select(Retentie, everything()) 

## Bekijk de data
## glimpse(dfOpleiding_inschrijvingen) 

## Laad dlookr
suppressMessages(library(dlookr))

## Maak een diagnose van de data
diagnose(dfOpleiding_inschrijvingen) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(unique_rate, 2)) |>
  knitr::kable(col.names = c("Variabelen",
                           "Type",
                           "# Missende waarden",
                           "% Missende waarden",
                           "# Unieke waarden",
                           "Ratio unieke waarden"))

detach("package:dlookr", unload = TRUE)

```

### Bekijk de onderlinge correlaties

Het is verstandig om voorafgaand aan het bouwen van een model te kijken naar de onderlinge correlaties tussen numerieke variabelen. Dit geeft inzicht in de data en kan helpen bij het maken van keuzes voor het model of de duiding van de uitkomsten.

```{r}
#| label: fig-corplot-data
#| fig-cap: "Correlatiematrix"

## Maak een plot van de onderlinge correlaties in numerieke variabelen
## Verwijder de kolommen met een standaarddeviatie van 0
dfOpleiding_inschrijvingen |> 
  select(-Collegejaar) |>
  select(where(is.numeric)) |> 
  select_if(~ sd(.) != 0) |>
  cor() |> 
  corrplot::corrplot(
    order = 'hclust', 
    addrect = 4,
    method = "number",  
    tl.cex = 0.8,       
    tl.col = "black",
    diag = FALSE)
```

### Bouw de trainingset, validatieset en testset

-   De data is nu geschikt om een prognosemodel mee te bouwen.
-   Om het model te bouwen, testen en valideren, splitsen we de data in drie delen van 60%, 20% en 20%. We doen dit op zo'n manier, dat elk deel ongeveer een gelijk aantal studenten bevat dat doorstudeert (dus niet uitvalt).
-   We trainen het model op basis van 60% en valideren de modellen tijdens het trainen op de overige 20% (de validatieset).
-   De verdeling van de training- en validatieset muteren we 10x (10 *folds*) om te voorkomen dat het model te veel leert van de trainingset en daardoor slecht presteert op de validatieset.
-   Als het model klaar is, testen we het op de 20% studenten uit de testset. De testset blijft dus de gehele tijd ongemoeid, zodat we overfitting - een te goed model op bekende data, maar slechte presetaties (*performance*) op onbekende data - voorkomen.
-   Een willekeurig, maar vaststaand *seed*-getal voorkomt dat we bij elke run van het model c.q. deze code een net iets andere uitkomst krijgen.

```{r}
#| label: fig-dataset-split
#| fig-cap: "Splitsing van de dataset in trainingset, validatieset en testset"
#| out-width: 80%
knitr::include_graphics(here::here("R/images", "voorspelmodel-dataset-lta-hhs.png"))
```


```{r}
#| label: split-data

set.seed(0821)

## Splits de data in 3 delen: 60%, 20% en 20%
splits      <- initial_validation_split(dfOpleiding_inschrijvingen,
                                        strata = Retentie,
                                        prop = c(0.6, 0.2))

## Maak drie sets: een trainingset, een testset en een validatieset
dfRetentie_train      <- training(splits)
dfRetentie_test       <- testing(splits)
dfRetentie_validation <- validation_set(splits)

## Maak een resample set op basis van 10 folds (default)
dfRetentie_resamples  <- vfold_cv(dfRetentie_train, strata = Retentie)
```

```{r, echo=FALSE}
#| label: tbl-split-data
#| tbl-cap: "Verhouding van de uitkomstvariabele in de training- en testset"

## Training set proporties
dfRetentie_train_prop <- dfRetentie_train |> 
  count(Retentie) |> 
  mutate(Naam = "Trainingset",
         prop = n/sum(n)) 

## Test set proporties
dfRetentie_test_prop <- dfRetentie_test  |> 
  count(Retentie) |> 
  mutate(Naam = "Testset",
         prop = n/sum(n)) 

## Combineer de training- en testmutaspkcap"set om te tonen in een tabel
bind_rows(dfRetentie_train_prop,
          dfRetentie_test_prop) |> 
  mutate(prop = scales::percent(prop, accuracy = 0.1)) |>
  select(Naam, Retentie, n, prop) |> 
  knitr::kable(col.names = c("Naam", "Retentie", "Aantal", "Proportie"))

```

<!-- MODEL I: Penalized Logistic Regression -->

::: {.content-hidden unless-meta="includes.model_lr"}
## Model I: Logistische Regressie

-   Het eerste model is een [logistische regressie met penalized likelihood](https://wikistatistiek.amc.nl/Logistische_regressie); we gebruiken de `glmnet` engine voor het bouwen van het model. Penalized likelihood is een techniek die helpt bij het voorkomen van overfitting: het voegt voor elke extra variabele een strafterm toe om eenvoudige modellen te belonen. [Glmnet](https://glmnet.stanford.edu/articles/glmnet.html) is een veelgebruikt package voor het bouwen van logistische regressiemodellen.
-   We gebruiken de [Area under the ROC Curve (AUC/ROC)](https://nl.wikipedia.org/wiki/ROC-curve) als performance metric. De ROC-curve (Receiver Operating Characteristic) is een grafiek die de prestaties van een classificatiemodel afbeeldt door de verhouding tussen de *true positives* (sensitiviteit) en de *false positives* (aspecficiteit = 1-specificiteit) te plotten bij verschillende drempelwaarden. De oppervlakte onder deze curve, bekend als de AUC (Area Under the Curve), kwantificeert het onderscheidingsvermogen van het model; een AUC van 1 duidt op een perfect onderscheidend vermogen, terwijl een AUC van 0,5 wijst op een model zonder onderscheidend vermogen.

### Maak het model

Eerst bouwen we het model.

```{r}
#| label: lr-mod
#| code-fold: false

## Bouw het model: logistische regressie
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")
```

### Maak de recipe

Vervolgens zetten we meerdere stappen in een 'recipe':

-   We definiÃ«ren de student-ID als ID variabele. Daarmee krijgt deze variabele de rol van uniek rij-kenmerk.
-   We verwijderen vervolgens de oorspronkelijke student-ID en het collegejaar uit de data, omdat deze verder niet gebruikt moeten worden in het model.
-   We converteren factoren naar dummy variabelen: voor elke categorie wordt er een nieuwe logische variabele (Ja/Nee) aangemaakt.
-   We verwijderen variabelen die geen waarde toevoegen: variabelen met uitsluitend nullen.
-   We normaliseren numerieke variabelen om ze met elkaar te kunnen vergelijken door ze te centreren en schalen.
-   Sterk gecorreleerde waarden verwijderen we nu niet, omdat we later in de analyse de eventuele samenhang met andere variabelen in een prognosemodel nog willen kunnen visualiseren.

```{r}
#| label: lr-recipe
#| code-fold: false

## Bouw de recipe: logistische regressie
lr_recipe <- 
  recipe(Retentie ~ ., data = dfRetentie_train) |>  
  update_role(ID, new_role = "ID") |>           ## Zet de student ID als ID variabele
  step_rm(ID, Collegejaar) |>                   ## Verwijder ID en collegejaar uit het model
  step_dummy(all_nominal_predictors()) |>       ## Maak dummy variabelen van categorische variabelen
  step_zv(all_predictors()) |>                  ## Verwijder zero values
  step_normalize(all_numeric_predictors())      ## Centreer en schaal numerieke variabelen

```

```{r}
#| label: tbl-lr-recipe-steps
#| tbl-cap: "Recipesteps voor logistische regressie"

## Toon de recipe
tidy(lr_recipe) |> 
  knitr::kable(col.names = c("Nummer", 
                             "Operatie", 
                             "Type",
                             "Getraind",
                             "Sla over",
                             "ID"))
```

De variabelen die nu nog overblijven zijn:

```{r, echo=FALSE}
#| label: tbl-lr-recipe-final
#| tbl-cap: "Resterende variabelen voor logistische regressie na bewerkingen"

## Toon de variabelen die nog resteren
model_vars <- lr_recipe |> 
  prep() |> 
  juice() |> 
  names()

## Voeg lege waarden toe om de lengte deelbaar door 3 te maken
while (length(model_vars) %% 3 != 0) {
  model_vars <- c(model_vars, "")
}

model_vars_matrix <- matrix(model_vars, ncol = 3, byrow = FALSE)

knitr::kable(model_vars_matrix)

```

### Maak de workflow

Voor de uitvoering bouwen we een workflow. Daaraan voegen we het model en de bewerkingen in de recipe toe.

```{r}
#| label: lr-workflow
#| code-fold: false

## Maak de workflow: logistische regressie
lr_workflow <- 
  workflow() |>         ## Maak een workflow
  add_model(lr_mod) |>  ## Voeg het model toe
  add_recipe(lr_recipe) ## Voeg de recipe toe

## Toon de workflow
lr_workflow
```

### Tune en train het model

Het model moet getuned worden. Dit houdt in dat we de beste parameters voor het model moeten vinden. We maken een grid met verschillende penalty waarden. Daarmee kunnen we vervolgens het beste model selecteren met de hoogste ROC/AUC. We plotten de resultaten van de tuning, zodat we hieruit het beste model kunnen kiezen.

```{r}
#| label: lr-reg-grid
#| code-fold: false

## Maak een grid: logistische regressie
lr_reg_grid <- tibble(penalty = 10 ^ seq(-4, -1, length.out = 30))

## Train en tune het model: logistische regressie
lr_res <- 
  lr_workflow |> 
  tune_grid(dfRetentie_validation,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```

```{r}
#| label: fig-lr-plot
#| fig-cap: "Tuning resultaten logistische regressie"

## Plot de resultaten + een rode verticale lijn voor de max AUC
lr_plot <- 
  lr_res |> 
  collect_metrics() |> 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  
  ## Maak de schaal van de x-as logaritmisch
  scale_x_log10(labels = scales::label_number()) +
    theme(
      axis.title.x = element_text(margin = margin(t = 20))
    ) +
  
  # Bepaal de titel, ondertitel en caption
  labs(
    caption = sCaption,
    x = "Area under the ROC Curve",
    y = "Penalty"
  )
  
  ## Voeg LTA elementen toe
  lr_plot <- Add_LTA_Theme_Elements(lr_plot, title_subtitle = FALSE)
  
# Zoek de penalty waarde met de max AUC
max_auc_penalty <- lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |> 
  pull(penalty)

# Voeg de rode verticale lijn toe aan lr_plot
lr_plot_plus <- lr_plot + 
  geom_vline(xintercept = max_auc_penalty, color = "red")

# Vind een mean voor de max AUC die hoger is
max_auc_mean <- lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |> 
  pull(penalty)

## Print de definitieve plot
lr_plot_plus

```

### Kies het beste model

De prestaties van een model gevisualiseerd met behulp van een ROC curve. De sensitiviteit (True Positive Rate) en specificiteit (True Negative Rate) worden hierin tegenover elkaar uitgezet. De Area under the ROC Curve (AUC/ROC) geeft de prestaties van het model weer. Het model scoort beter naarmate de AUC/ROC dichter bij de 1 ligt, de linker bovenhoek. De linker bovenhoek houdt in dat alle prognoses exact overeenstemmen met de werkelijkheid. Een AUC/ROC van 0,5 betekent dat het model niet beter presteert dan een willekeurige voorspelling.

We gebruiken modellen met een zo hoog mogelijke Area under the ROC Curve (AUC/ROC) en een zo laag mogelijke penalty. Zo kunnen we uit de resultaten het beste model kiezen en visualiseren.

```{r}
#| label: lr-top-models
#| code-fold: false

## Toon het beste model
top_models <-
  lr_res |> 
  show_best(metric = "roc_auc", n = 10) |> 
  mutate(mean = round(mean, 6)) |>
  arrange(penalty) 
```

```{r}
#| label: tbl-lr-top-models
#| tbl-cap: "Model performance voor logistische regressie"

top_models|> 
  knitr::kable(col.names = c("Penalty", 
                             "Metriek", 
                             "Estimator",
                             "Gemiddelde",
                             "Aantal",
                             "SE",
                             "Configuratie"))

```

```{r}
#| label: lr-best
#| code-fold: false

## Selecteer het beste model: logistische regressie
lr_best <- 
  lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |>
  slice(1) 
```

```{r}
#| label: tbl-lr-best
#| tbl-cap: "Hoogste model performance voor logistische regressie"

lr_best|> 
  mutate(mean = round(mean, 6)) |>
  knitr::kable(col.names = c("Penalty", 
                             "Metriek", 
                             "Estimator",
                             "Gemiddelde",
                             "Aantal",
                             "SE",
                             "Configuratie"))

```

```{r}
#| label: lr-auc
#| code-fold: false

## Verzamel de predicties en evalueer het model (AUC/ROC): logistische regressie
lr_auc <- 
  lr_res |> 
  collect_predictions(parameters = lr_best) |> 
  roc_curve(Retentie, .pred_FALSE) |> 
  mutate(model = "Logistisch Regressie")
```
```{r}
#| label: fig-lrauc
#| fig-cap: "ROC curve voor logistische regressie"

## Plot de ROC curve
Get_ROC_Plot(lr_auc, position = 1)
```

```{r}
#| label: lr-auc-highest

## Bepaal de AUC van het beste model
lr_auc_highest   <-
  lr_res |>
  collect_predictions(parameters = lr_best) |> 
  roc_auc(Retentie, .pred_FALSE)

## Voeg de naam van het model en de AUC toe dfModel_results
dfModel_results <- 
  dfModel_results |>
  add_row(model = "Logistic Regression", auc = lr_auc_highest$.estimate)

```
:::

<!-- MODEL II: Random Forest -->

::: {.content-hidden unless-meta="includes.model_rf"}
## Model II: Tree-based ensemble

-   Het tweede model is een [random forest](https://en.wikipedia.org/wiki/Random_forest): een ensemble van beslisbomen (*decision trees*). Het is een krachtig model dat goed om kan gaan met complexe data en veel variabelen.
-   We gebruiken de [`ranger` engine](https://cran.r-project.org/web/packages/ranger/index.html) voor het bouwen van het model.

### Bepaal het aantal PC-cores

Omdat een random forest model veel berekeningen vereist, willen we daarvoor alle computerkracht gebruiken die beschikbaar is. Het aantal CPU's (*cores*), wat verschilt per computer, bepaalt hoe snel het model getraind kan worden. We bepalen het aantal cores en gebruiken dat bij het bouwen van het model.

```{r}
#| label: cores

## Bepaal het aantal cores
cores <- parallel::detectCores()

```

### Maak het model

We bouwen eerst het model. We gebruiken de `rand_forest` functie om het model te bouwen. We tunen de `mtry` en `min_n` parameters. De `mtry` parameter bepaalt het aantal variabelen dat per boom wordt gebruikt. De `min_n` parameter bepaalt het minimum aantal observaties dat in een blad van de boom moet zitten. De functie `tune()` is hier nog een *placeholder* om de beste waarden voor deze parameters - die we later bepalen - in te kunnen stellen. We gebruiken 1.000 bomen c.q. versies van het model.

```{r}
#| label: rf-mod
#| code-fold: false

## Bouw het model: random forest

rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |> 
  set_engine("ranger", num.threads = cores) |> 
  set_mode("classification")
```

### Maak de recipe

We maken een recipe voor het random forest model. We verwijderen de student ID en het collegejaar uit de data, omdat deze niet moet worden gebruikt in het model. Overige stappen zijn bij een random forest minder relevant in tegenstelling tot een regressiemodel.

```{r}
#| label: rf-recipe
#| code-fold: false

## Maak de recipe: random forest
rf_recipe <- 
  recipe(Retentie ~ ., data = dfRetentie_train) |> 
  step_rm(ID, Collegejaar)                      ## Verwijder ID en Collegejaar uit het model
```

```{r}
#| label: tbl-rf-recipe
#| tbl-cap: "Recipesteps voor random forest"

## Toon de recipe
tidy(rf_recipe) |> 
  knitr::kable(col.names = c("Nummer", 
                             "Operatie", 
                             "Type",
                             "Getraind",
                             "Sla over",
                             "ID"))
```

### Maak de workflow

We voegen het model en de recipe toe aan de workflow voor dit model.

```{r}
#| label: rf-workflow
#| code-fold: false

## Maak de workflow: random forest
rf_workflow <- 
  workflow() |> 
  add_model(rf_mod) |> 
  add_recipe(rf_recipe)

## Toon de workflow
rf_workflow
```

### Tune en train het model

We trainen en tunen het model in de workflow. We maken een grid met verschillende waarden voor de parameters `mtry` en `min_n`. We gebruiken de Area under the ROC Curve (AUC/ROC) als performance metric. Met de resultaten van de tuning kiezen we het beste model.

```{r}
#| label: rf-tune
#| code-fold: false

## Toon de parameters die getuned kunnen worden
rf_mod

## Extraheer de parameters die getuned worden
extract_parameter_set_dials(rf_mod)

## Bepaal de seed
set.seed(2904)

## Bouw het grid: random forest
rf_res <- 
  rf_workflow |> 
  tune_grid(dfRetentie_validation,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

### Kies het beste model

We evalueren de beste modellen en maken een ROC curve om de performance van het model te visualiseren. Vervolgens vergelijken we de prestaties van de modellen en kiezen daaruit het beste model.

```{r}
#| label: tbl-rf-results
#| tbl-cap: "Model performance voor random forest"
#| code-fold: false

## Toon de beste modellen
rf_res |> 
  show_best(metric = "roc_auc", n = 15) |> 
  mutate(mean = round(mean, 6)) |>
  knitr::kable(col.names = c("Mtry", 
                             "Min. aantal", 
                             "Metriek",
                             "Estimator",
                             "Gemiddelde",
                             "Aantal",
                             "SE",
                             "Configuratie"))

```
```{r}
#| label: fig-rf-results
#| fig-cap: "Model performance random forest"

## Plot de resultaten
autoplot <- autoplot(rf_res) +
  theme_minimal() +
  labs(
    y = "roc/auc",
    caption = sCaption
  )
  
  ## Voeg LTA elementen toe
  autoplot <- Add_LTA_Theme_Elements(autoplot, title_subtitle = FALSE)
  
  print(autoplot)

```

```{r}
#| label: rf-best
#| code-fold: false

## Selecteer het beste model
rf_best <- 
  rf_res |> 
  select_best(metric = "roc_auc")

```

```{r}
#| label: tbl-ref-best
#| tbl-cap: "Hoogste model performance voor random forest"

rf_best|> 
  knitr::kable(col.names = c("Mtry", 
                             "Min. aantal", 
                             "Configuratie"))

```

```{r}
#| label: tbl-rf-predictions
#| tbl-cap: "Predicties voor random forest"
#| code-fold: true

## Verzamel de predicties
rf_res |> 
  collect_predictions() |> 
  head(10) |>
  mutate(.pred_FALSE = scales::percent(.pred_FALSE, accuracy = 0.1),
         .pred_TRUE = scales::percent(.pred_TRUE, accuracy = 0.1)) |>
  knitr::kable(col.names = c("% Voorsp. FALSE", 
                             "% Voorsp. TRUE", 
                             "ID",
                             "Rij",
                             "Mtry", 
                             "Min. aantal", 
                             "Retentie",
                             "Configuratie"))
```

```{r}
#| label: fig-rf-auc
#| fig-cap: "ROC curve voor random forest"
 
## Bepaal de AUC/ROC curve
rf_auc <- 
  rf_res |> 
  collect_predictions(parameters = rf_best) |> 
  roc_curve(Retentie, .pred_FALSE) |> 
  mutate(model = "Random Forest")

## Plot de ROC curve
Get_ROC_Plot(rf_auc, position = 2)

## Bepaal de AUC van het beste model
rf_auc_highest   <-
  rf_res |>
  collect_predictions(parameters = rf_best) |> 
  roc_auc(Retentie, .pred_FALSE)

## Voeg de naam van het model en de AUC toe dfModel_results
dfModel_results <- 
  dfModel_results |>
  add_row(model = "Random Forest", auc = rf_auc_highest$.estimate)

```
:::

<!-- Final Fit -->

## De uiteindelijke fit

-   In de laatste stap van deze analyse maken we het model definitief.
-   We testen het model op de testset en evalueren het model met metrieken en de Variable Importance (VI). De VI kwantificeert de bijdrage van elke variabele aan de voorspellende kracht van een model. Het identificeert welke variabelen significant zijn voor de modelprestaties, wat essentieel is voor het interpreteren en optimaliseren van een model [@VanderLaan.2006]. Methoden zoals de Shapley-waarde en permutation importance worden vaak toegepast om dit belang te meten. Op deze methoden komen we terug in het volgende hoofdstuk.

### Combineer de AUC/ROC curves en kies het beste model

Eerst combineren we de AUC/ROC curves van de modellen om ze te vergelijken. We kiezen het beste model op basis van de hoogste AUC/ROC.

```{r}
#| label: fig-bind-rows-auc-roc
#| fig-cap: "Gecombineerde ROC curves"

## Combineer de AUC/ROC curves om de modellen te vergelijken
Get_ROC_Plot(list(lr_auc, rf_auc))

```

```{r}
#| label: best-model-auc-roc

## Bepaal welke van de modellen het beste is op basis van de hoogste AUC/ROC
dfModel_results <- dfModel_results |>
  mutate(number = row_number()) |> 
  mutate(best = ifelse(auc == max(auc), TRUE, FALSE)) |> 
  arrange(number)

## Bepaal het beste model
sBest_model     <- dfModel_results$model[dfModel_results$best == TRUE]
sBest_model_auc <- round(dfModel_results$auc[dfModel_results$best == TRUE], 4)
```

```{r, echo=FALSE, results='asis'}
#| label: best-model-auc-roc-text

## Bouw de tekst voor de beste modellen op
sText_best_model <- ""

walk(1:nrow(dfModel_results), function(i) {
  sText_best_model <-
    glue(
      sText_best_model,
      "Het {dfModel_results[i,]$model} model heeft een AUC van {round(dfModel_results[i,]$auc, 4)}. "
    )
})

## Herschik de uiteindelijke tekst voor een heldere samenvatting
sText_best_model <-
  glue(
    "Het beste model is het **{sBest_model}** model met een **AUC/ROC van {sBest_model_auc}**. {sText_best_model} We ronden de analyse verder af met het {sBest_model} model."
  )

```

`r sText_best_model`

### Maak het finale model

```{r, echo=FALSE, results='asis'}
#| label: text-final-model

## Bouw de tekst voor het laatste model op
if(sBest_model == "Logistisch Regressie") {
  
  ## Maak het laatste model
  sText_final_model <- "We maken het finale model op basis van de beste parameters die we hebben gevonden. Voor het Logistisch Regressie model is dit het model met de beste penalty en mixture."

} else if (sBest_model == "Random Forest") {
  
  ## Maak het laatste model
  sText_final_model <- "We maken het finale model op basis van de beste parameters die we hebben gevonden. Door in de engine bij `importance` de `impurity` op te geven, wordt het beste random forest model gekozen om de data definitief mee te classificeren."

}
```

We maken het finale model op basis van de beste parameters die we hebben gevonden. Door in de engine bij `importance` de `impurity` op te geven, wordt het beste random forest model gekozen om de data definitief mee te classificeren.

```{r}
#| label: last-mod
#| code-fold: false

## Test het ontwikkelde model op de testset
## Bepaal de optimale parameters

## Bouw de laatste modellen
last_lr_mod <-
    logistic_reg(penalty = lr_best$penalty,
                 mixture = 1) |>
    set_engine("glmnet") |>
    set_mode("classification")

last_rf_mod <-
    rand_forest(mtry = rf_best$mtry,
                min_n = rf_best$min_n,
                trees = 1000) |>
    set_engine("ranger", num.threads = cores, importance = "impurity") |>
    set_mode("classification")

```

### Maak de workflow

We voegen het model toe aan de workflow en updaten de workflow met het finale model.

```{r}
#| label: last-workflow
#| code-fold: false

## Update de workflows
 last_lr_workflow <- 
    lr_workflow |> 
    update_model(last_lr_mod)

 last_rf_workflow <- 
    rf_workflow |> 
    update_model(last_rf_mod)

```

### Fit het finale model

We voeren de finale fit uit. De functie `last_fit` past het model toe op de validatieset.

```{r}
#| label: last-fit
#| code-fold: false

## Voer de laatste fit uit
set.seed(2904)

## Maak voor beide modellen een laatste fit, zodat we deze kunnen opslaan voor later gebruik
last_fit_lr <- 
    last_lr_workflow |> 
    last_fit(splits)

last_fit_rf <- 
    last_rf_workflow |> 
    last_fit(splits)

lLast_fits <- list(last_fit_lr, last_fit_rf) |> 
  set_names(c("Logistic Regression", "Random Forest"))

## Bepaal welk model het beste is
if(sBest_model == "Logistic Regression") {
  last_fit <- last_fit_lr
} else if(sBest_model == "Random Forest") {
  last_fit <- last_fit_rf
}

## Bewaar de resultaten, de modelresultaten en de bijbehorende data
sFittedmodels_outputpath <- Get_Model_Outputpath(mode = "last-fits")
saveRDS(lLast_fits, file = sFittedmodels_outputpath)

sModelresults_outputpath <- Get_Model_Outputpath(mode = "modelresults")
saveRDS(dfModel_results, file = sModelresults_outputpath)

sData_outputpath <- Get_Model_Outputpath(mode = "data")
saveRDS(dfOpleiding_inschrijvingen, file = sData_outputpath)

```

### Evalueer het finale model: metrieken en variable importance

We evalueren het finale model op basis van 4 metrieken: 1) accuraatheid, 2) ROC/AUC en 3) de [Brier score](https://en.wikipedia.org/wiki/Brier_score) (de Mean Squared Error) en 4) de Variable Importance (VI). Uit de VI is op te maken welke variabelen het meest bijdragen aan de voorspelling van de uitkomstvariabele.

```{r}
#| label: last-fit-metrics-vi
#| code-fold: false

## Verzamel de metrieken
last_fit |> 
  collect_metrics() |> 
  mutate(.estimate = round(.estimate, 4)) |>
  knitr::kable(col.names = c("Metriek", 
                             "Estimator",
                             "Estimate",
                             "Configuratie"))

```

```{r}
#| label: fig-last-fit-metrics-vi
#| fig-cap: "Meest voorspellende factoren op basis van de Variable Importance (VI)"
#| code-fold: true

# Extraheer de feature importance
dfVi <- last_fit |>
  extract_fit_parsnip() |>
  vip::vi() |> 
  arrange(desc(Importance)) |>
  head(20)
  
# Maak de plot met fill op de variabele 'Importance'
importance_plot <- dfVi |> 
  ggplot(aes(x = reorder(Variable, Importance), 
             y = Importance, 
             fill = Importance)) +
  geom_col(show.legend = FALSE) +
  
  ## Maak de titel en caption
  labs(x = NULL,
       y = "VI-score",
       caption = sCaption) +
  
  theme_minimal() +
  Set_LTA_Theme() +
  
  theme(
    axis.title.x = element_text(margin = margin(t = 20))
  ) +
  
  coord_flip()
  
  ## Voeg LTA elementen toe
  importance_plot <- Add_LTA_Theme_Elements(importance_plot, title_subtitle = TRUE)

# Toon de plot
print(importance_plot)

```

### Plot de ROC curve

Tot slot visualiseren we de prestaties weer met een ROC curve van het beste model.

```{r}
#| label: fig-last-fit-roc
#| fig-cap: "ROC curve finale model"
#| code-fold: false

## Toon de roc curve
auc_lf <- last_fit |> 
  collect_predictions() |> 
  roc_curve(Retentie, .pred_FALSE) |> 
  mutate(model = "Last fit")

Get_ROC_Plot(auc_lf, position = 3)

```

<!-- Conclusies -->

::: {.content-hidden unless-meta="includes.conclusies"}
## Conclusies

```{r, echo = FALSE}
#| label: conclusions-accuracy
#| code-fold: false

## Bepaal de accuraatheid van het model, het gemiddelde retentiepercentage en het base-model
Last_fit_Accuracy   <- last_fit |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  pull(.estimate) |>
  round(4) * 100
Avg_Retentie        <- round(mean(dfOpleiding_inschrijvingen$Retentie == TRUE) * 100, 2)
Avg_Non_Retentie    <- round(100 - Avg_Retentie, 2)

if(Avg_Retentie < 50) {
  Base_Model_Accuracy <- round(100 - Avg_Retentie, 2)
  sRetentie <- "die niet doorstudeerde"
} else {
  Base_Model_Accuracy <- Avg_Retentie
  sRetentie <- "die doorstudeerde"
}

## Bereken nu het verschil in accuraatheid
nAccuracy_verschil  <- round(abs(Last_fit_Accuracy - Base_Model_Accuracy), 2)
pAccuracy_verschil  <- paste0(nAccuracy_verschil, "%") 

## Functies
Get_Accuracy_vergelijking <-
  function(Last_fit_Accuracy, Base_Model_Accuracy) {
    if (Last_fit_Accuracy == Base_Model_Accuracy) {
      "even goed als"
    } else if (Last_fit_Accuracy > Base_Model_Accuracy) {
      "beter"
    } else {
      "slechter"
    }
  }
  
Get_Accuracy_niveau <- function(Last_fit_Accuracy) {
  if (Last_fit_Accuracy > 95) {
    "zeer hoog"
  } else if (Last_fit_Accuracy > 90) {
    "hoog"
  } else if (Last_fit_Accuracy > 80) {
    "vrij hoog"
  } else if (Last_fit_Accuracy > 70) {
    "gemiddeld"
  } else if (Last_fit_Accuracy > 60) {
    "vrij laag"
  } else {
    "laag"
  }
}
  
Get_Accuracy_verschil <- function(nAccuracy_verschil) {
  if (nAccuracy_verschil < 5) {
    "iets"
  } else if (nAccuracy_verschil < 10) {
    "wat"
  } else if (nAccuracy_verschil < 20) {
    "een stuk"
  } else {
    "veel"
  }
}

## Functies om zinnen te maken en variabelen te printen
Special_Paste  <- function(vec) sub(",\\s+([^,]+)$", " en \\1", toString(vec))
Print_Variable <- function(variable) paste0("`", variable, "`")

## Bepaal een aantal teksten
sAccuracy_vergelijking <- Get_Accuracy_vergelijking(Last_fit_Accuracy, Base_Model_Accuracy)
sAccuracy_niveau       <- Get_Accuracy_niveau(Last_fit_Accuracy)
sAccuracy_verschil     <- Get_Accuracy_verschil(nAccuracy_verschil)

## Bepaal de top 3 en top 5 variabelen op basis van de VI
dfTop_vi   <- last_fit |> 
  extract_fit_parsnip() |> 
  vip::vi() |> 
  arrange(desc(Importance)) 

lTop3_vi <- dfTop_vi |>
  slice(1:3) |>
  pull(Variable) |>
  ## Pas de variabelen aan zodat ze tussen backticks staan
  Print_Variable() |>
  ## Maak er een zin van
  Special_Paste()  

lTop45_vi <- dfTop_vi |>
  ## Regel 4 en 5
  slice(4:5) |>
  pull(Variable) |>
  ## Pas de variabelen aan zodat ze tussen backticks staan
  Print_Variable() |>
  ## Maak er een zin van
  Special_Paste()

sConclusie_text <- ""

if(!("Eindcijfer_VO" %in% dfTop_vi$Variable[1:3]) 
   & "Eindcijfer_VO" %in% dfTop_vi$Variable[4:5]) {
  sConclusie_text <- glue::glue(
    sConclusie_text,
    "**Het model toont wel aan dat de herkomst van studenten sterker is gecorreleerd met {tolower(sSucces_model)} dan eerdere prestaties of vooropleiding.** ",
    "De _Variable Importance Factor_ (VI) laat namelijk zien dat de variabelen {lTop3_vi} de 3 belangrijkste variabelen zijn voor het voorspellen van {tolower(sUitval_model)}, gevolgd door {lTop45_vi}."
  )
} else if ("Eindcijfer_VO" %in% dfTop_vi$Variable[1:3]) {
  sConclusie_text <- glue::glue(
    sConclusie_text,
    "**Het model toont aan dat eerdere prestaties van studenten sterker zijn gecorreleerd met {tolower(sSucces_model)} dan herkomst.** ",
    "De _Variable Importance_ (VI) laat namelijk zien dat de variabelen {lTop3_vi} de 3 belangrijkste variabelen zijn voor het voorspellen van {tolower(sSucces_model)}, gevolgd door {lTop45_vi}."
  )
} else {
  sConclusie_text <- "**Vooralsnog geen nadere bijzonderheden.**"
}

```

### Het beste prognosemodel voor deze opleiding

**Het beste prognosemodel blijkt het `r sBest_model` model te zijn.**

-   Van de prognosemodellen die we hebben ontwikkeld om `r tolower(sSucces_model)` te voorspellen, had het `r sBest_model` model de hoogste AUC/ROC waarde (`r sBest_model_auc`).

### Mate van accuraatheid en lift

Een prognosemodel moet minimaal beter presteren dan een *base-model* om waarde op basis van accuraatheid toe te voegen. Het base-model neemt als basis de grootste klasse van de gemiddelde `r tolower(sSucces_model)` van de afgelopen jaren. Stel we zouden tegen alle studenten zeggen dat ze hun studie gaan halen, dan is de mate van accuratesse gelijk aan dit base-model. Dit base-model is dus altijd hoger dan de 50% lijn van de AUC/ROC curve, tenzij het base-model toevallig precies 50% is.


```{r}
#| label: fig-basemodel-lift
#| fig-cap: "Lift afhankelijk van base-model en accuraatheid"
#| out-width: 80%
knitr::include_graphics(here::here("R/images", "basemodel-lift.png"))
```

**De mate van accuraatheid van het prognosemodel is `r sAccuracy_niveau` (`r Last_fit_Accuracy`%).**

-   **Base-model: `r Base_Model_Accuracy`%** -- Voor deze opleiding berekenen we het base-model als volgt. Van alle studenten studeerde `r Avg_Retentie`% door; 100% - `r Avg_Retentie`% = `r Avg_Non_Retentie`% studeerde *niet* door. De grootste klasse van deze twee, `r Base_Model_Accuracy`%, is daarmee de accuratesse van het base-model.
-   **Accuratesse prognose: `r Last_fit_Accuracy`%** -- Het model voorspelt `r sSucces_model` met een accuratesse van `r Last_fit_Accuracy`%.
-   **Lift: `r pAccuracy_verschil`** -- Het model scoort in de huidige opbouw met een verschil van `r pAccuracy_verschil` (de *lift*) `r sAccuracy_verschil` `r sAccuracy_vergelijking` dan de accuraatheid van het base-model.

### Confusion Matrix

```{r}
#| label: confusion-matrix-calculation

## Bepaal de confusion matrix
confusion_matrix <- last_fit |>
  collect_predictions() |>
  conf_mat(truth = Retentie, estimate = .pred_class) 

dfConf_matrix <- as_tibble(confusion_matrix$table) |>
  rename(Werkelijkheid = Truth) |>
  mutate(Werkelijkheid = ifelse(Werkelijkheid == "TRUE", "Retentie", "Geen retentie"),
         Prediction    = ifelse(Prediction == "TRUE", "Retentie", "Geen retentie"))

pTP  <- Change_Number_Marks((dfConf_matrix$n[4]/sum(dfConf_matrix$n)*100),1)
pFP  <- Change_Number_Marks((dfConf_matrix$n[2]/sum(dfConf_matrix$n)*100),1)
pTN  <- Change_Number_Marks((dfConf_matrix$n[1]/sum(dfConf_matrix$n)*100),1)
pFN  <- Change_Number_Marks((dfConf_matrix$n[3]/sum(dfConf_matrix$n)*100),1)
pACC <- Change_Number_Marks(Last_fit_Accuracy,1)


```

De prestaties van het model kunnen we verder uitdrukken in een *confusion matrix*. Hierin zien we de voorspellingen van het model en de werkelijke uitkomsten. De matrix geeft inzicht in de mate van correcte en incorrecte voorspellingen. Ter illustratie werken we de matrix uit voor een voorspelling waarop een bindend studieadvies (BSA) gebaseerd zou kunnen zijn.

```{r}
#| label: fig-confusion-matrix-explanation
#| fig-cap: "Confusion matrix in relatie tot BSA"
knitr::include_graphics(here::here("R/images", "confusion-matrix-retention-lta-hhs.png"))
```

We passen de confusion matrix nu toe op het model dat als beste naar voren kwam. De **accuraatheid** van dit model is **`r pACC`%**. De accuraatheid van het model berekenen we door de som van de diagonaal te berekenen: het aandeel goed voorspelde uitkomsten, Retentie = Retentie (*True Positive*) en Geen retentie = Geen retentie (*True Negative*), af te zetten tegen het totaal aantal voorspellingen: `r pTP`% + `r pTN`% = `r pACC`%. (NB. De weergave in deze confusion matrix is diagonaal gespiegeld vergeleken met het voorbeeld.)

```{r}
#| label: fig-confusion-matrix-actual
#| fig-cap: !expr 'paste("Confusion matrix ten opzichte van", params$succes)'
 
confusion_plot <- plot_confusion_matrix(
    dfConf_matrix,
    target_col = "Werkelijkheid",
    prediction_col = "Prediction",
    counts_col = "n",
    palette = "Blues",
    add_sums = TRUE,
    theme_fn = ggplot2::theme_light,
    sums_settings = sum_tile_settings(
      palette = "Greens",
      label = "Totaal",
      tc_tile_border_color = "black"
    )) +
    
    ## Pas de labels aan
    labs(
      x = "Werkelijke uitkost",
      y = "Voorspelde uitkomst",
      caption = sCaption
    ) +
    
    Set_LTA_Theme()
  
  ## Voeg LTA elementen toe
  confusion_plot <- Add_LTA_Theme_Elements(confusion_plot, 
                                           title_subtitle = TRUE)
  
  print(confusion_plot)
  
```

### Uitleggen of verklaren?

Naast de accuraatheid van het model is het ook belangrijk om te weten welke factoren het meest bijdragen aan de voorspelling van `r tolower(sSucces_model)`. Daarin gaat de vergelijking met de prestaties van het basemodel mank. Dat model geeft op geen enkele manier aan waarom een student een kans op succes heeft, anders dan - 'dit is gebruikelijk in deze opleiding'.

Ongeacht de mate van accuraatheid, is het voor onderzoek naar kansengelijkheid essentieel om te weten welke factoren het meest bijdragen aan de voorspelling van `r tolower(sSucces_model)`. Het gaat erom dat we het belang van de factoren in de voorspellingen kunnen begrijpen en duiden. Machine Learning is hiervoor uitstekend geschikt, omdat het de mogelijkheid biedt om de belangrijkste factoren en hun invloed te leren kennen [@Shmueli.2010; @Shmueli.2011].

<!-- ## Factoren -->

<!-- -   `r sConclusie_text` -->
:::

<!-- Next steps -->

::: {.content-hidden unless-meta="includes.nextsteps"}
## Vervolgstappen: Factoranalyse

De volgende stap (stap 2) is een verdiepende analyse van de mate waarin de **factoren** die we gevonden hebben van invloed zijn op `r sSucces_model`. We kijken naar de rangorde, of ze `r tolower(sSucces_model)` verhogen of juist verlagen en hoe stabiel de factoren zijn als we in andere volgordes aan het model toevoegen. Om het concreet te maken zullen we het model toepassen op een aantal fictieve studenten, die we opbouwen uit de meeste voorkomende waarden in deze opleiding. Dit is het onderwerp van analyse 2: de Factoranalyse.
:::

<!-- Verantwoording -->

::: {.content-hidden unless-meta="includes.verantwoording"}
{{< include R/scripts/Include_Verantwoording.qmd >}}
:::

<!-- Copyright -->

{{< include R/scripts/Include_Copyright.qmd >}}

<!-- Opschonen -->

```{r, echo = FALSE}
#| label: cleanup

## Datasets
rm(
  dfOpleiding_inschrijvingen,
  dfSummary,
  dfRetentie_test,
  dfRetentie_train,
  dfRetentie_validation,
  splits
  )

## Logistische regressie
if(bInclude_Model_LR) {
  rm(
    lr_auc,
    lr_auc_highest,
    lr_best,
    lr_mod,
    lr_plot,
    lr_recipe,
    lr_res,
    lr_workflow
  )
  # if(sBest_model == "lr") {
  #   rm(last_lr_mod,
  #      last_lr_workflow)
  # }
}

## Random Forest
if(bInclude_Model_RF) {
  rm(
    rf_auc,
    rf_auc_highest,
    rf_best,
    rf_mod,
    rf_recipe,
    rf_res,
    rf_workflow,
    last_rf_mod,
    last_rf_workflow
  )
  # if(sBest_model == "rf") {
  #   rm(last_rf_mod,
  #      last_rf_workflow)
  # }
}

## Final Fit
if(bInclude_Final_fit) {
  rm(
    last_fit
  )
}

## Conclusies
if(bInclude_Conclusies) {
  rm(
    top_models,
    dfTop_vi,
    lTop3_vi,
    lTop45_vi
    )
}

## Collect garbage
invisible(gc())
  
```
